{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f99d49",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e721ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdDepictor, Descriptors\n",
    "from rdkit.Chem import MACCSkeys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cffb529",
   "metadata": {},
   "source": [
    "#### Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b067f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b948a1",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d95416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of atoms where a new element gets a new index\n",
    "def create_atoms(mol):\n",
    "    atoms = [atom_dict[a.GetSymbol()] for a in mol.GetAtoms()]\n",
    "    return np.array(atoms)\n",
    "\n",
    "# format from_atomIDx : [to_atomIDx, bondDict]\n",
    "def create_ijbonddict(mol):\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "    return i_jbond_dict\n",
    "\n",
    "\n",
    "def create_fingerprints(atoms, i_jbond_dict, radius):\n",
    "    \"\"\"Extract the r-radius subgraphs (i.e., fingerprints)\n",
    "    from a molecular graph using WeisfeilerLehman-like algorithm.\"\"\"\n",
    "\n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        fingerprints = [fingerprint_dict[a] for a in atoms]\n",
    "\n",
    "    else:\n",
    "        vertices = atoms\n",
    "        for _ in range(radius):\n",
    "            fingerprints = []\n",
    "            for i, j_bond in i_jbond_dict.items():\n",
    "                neighbors = [(vertices[j], bond) for j, bond in j_bond]\n",
    "                fingerprint = (vertices[i], tuple(sorted(neighbors)))\n",
    "                fingerprints.append(fingerprint_dict[fingerprint])\n",
    "            vertices = fingerprints\n",
    "\n",
    "    return np.array(fingerprints)\n",
    "\n",
    "\n",
    "def create_adjacency(mol):\n",
    "    adjacency  = Chem.GetAdjacencyMatrix(mol)\n",
    "    n          = adjacency.shape[0]\n",
    "\n",
    "    adjacency  = adjacency + np.eye(n)\n",
    "    degree     = sum(adjacency)\n",
    "    d_half     = np.sqrt(np.diag(degree))\n",
    "    d_half_inv = np.linalg.inv(d_half)\n",
    "    adjacency  = np.matmul(d_half_inv,np.matmul(adjacency,d_half_inv))\n",
    "    return np.array(adjacency)\n",
    "\n",
    "\n",
    "def dump_dictionary(dictionary, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(dict(dictionary), f)\n",
    "\n",
    "\n",
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]\n",
    "\n",
    "\n",
    "def load_numpy(file_name):\n",
    "    return np.load(file_name + '.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e4b63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-26 15:11:01,742 — INFO — Assuming multitask since y has more than one dimension. If otherwise, explicitly set the mode to 'classification' or 'regression'!\n"
     ]
    }
   ],
   "source": [
    "# export smiles to a text file with the index of the labels where it is not 0\n",
    "\n",
    "from deepmol.loaders import CSVLoader\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"kegg_pathways_w_labels_v2.csv\", nrows=2)\n",
    "labels = dataset.columns[4:]\n",
    "# LOAD THE DATA\n",
    "loader = CSVLoader('kegg_pathways_w_labels_v2.csv', \n",
    "                   smiles_field='SMILES',labels_fields=labels)\n",
    "dataset = loader.create_dataset(sep=\",\")\n",
    "#get the index of the labels where it is not 0 per row \n",
    "classes = [np.where(row != 0)[0].tolist() for row in dataset.y]\n",
    "# smiles to txt file with the index of the labels where it is not 0\n",
    "with open('kegg_smiles.txt', 'w') as f:\n",
    "    for i, c in enumerate(classes):\n",
    "        f.write(f\"{dataset.smiles[i]}\\t{','.join(map(str, c))}\\n\")\n",
    "\n",
    "# kegg_pathways[\"SMILES\"].to_csv(\"kegg_smiles.txt\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98046865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<deepmol.datasets.datasets.SmilesDataset at 0x7f424c5163e0>,\n",
       " <deepmol.datasets.datasets.SmilesDataset at 0x7f355875afb0>,\n",
       " <deepmol.datasets.datasets.SmilesDataset at 0x7f3660f0a350>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read pickle \n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"splits.pkl\", \"rb\") as f:\n",
    "    splits = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34ab3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_with_smiles(file_prefix, dataset):\n",
    "    #get the index of the labels where it is not 0 per row\n",
    "    classes = [np.where(row != 0)[0].tolist() for row in dataset.y]\n",
    "    # smiles to txt file with the index of the labels where it is not 0\n",
    "    with open(f'{file_prefix}.txt', 'w') as f:\n",
    "        for i, c in enumerate(classes):\n",
    "            f.write(f\"{dataset.smiles[i]}\\t{','.join(map(str, c))}\\n\")\n",
    "\n",
    "def generate_features(file_prefix):\n",
    "    radius = 2\n",
    "\n",
    "    with open(f'{file_prefix}.txt', 'r') as f:\n",
    "        data_list = f.read().strip().split('\\n')\n",
    "\n",
    "    \"\"\"Exclude the data contains \".\" in the smiles, which correspond to non-bonds\"\"\"\n",
    "    data_list = list(filter(lambda x: '.' not in x.strip().split()[0], data_list))\n",
    "    N = len(data_list)\n",
    "\n",
    "    print('Total number of molecules : %d' %(N))\n",
    "\n",
    "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "\n",
    "    Molecules, Adjacencies, Properties, MACCS_list = [], [], [], []\n",
    "\n",
    "    max_MolMR, min_MolMR     = -1000, 1000\n",
    "    max_MolLogP, min_MolLogP = -1000, 1000\n",
    "    max_MolWt, min_MolWt     = -1000, 1000\n",
    "    max_NumRotatableBonds, min_NumRotatableBonds = -1000, 1000\n",
    "    max_NumAliphaticRings, min_NumAliphaticRings = -1000, 1000\n",
    "    max_NumAromaticRings, min_NumAromaticRings   = -1000, 1000\n",
    "    max_NumSaturatedRings, min_NumSaturatedRings = -1000, 1000\n",
    "\n",
    "    for no, data in enumerate(data_list):\n",
    "\n",
    "        print('/'.join(map(str, [no+1, N])))\n",
    "\n",
    "        smiles, property_indices = data.strip().split('\\t')\n",
    "        property_s = property_indices.strip().split(',')\n",
    "\n",
    "        property = np.zeros((1,21))\n",
    "        for prop in property_s:\n",
    "            property[0,int(prop)] = 1\n",
    "\n",
    "        Properties.append(property)\n",
    "\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atoms = create_atoms(mol)\n",
    "        i_jbond_dict = create_ijbonddict(mol)\n",
    "\n",
    "        fingerprints = create_fingerprints(atoms, i_jbond_dict, radius)\n",
    "        Molecules.append(fingerprints)\n",
    "\n",
    "        adjacency = create_adjacency(mol)\n",
    "        Adjacencies.append(adjacency)\n",
    "\n",
    "        MACCS         = MACCSkeys.GenMACCSKeys(Chem.MolFromSmiles(smiles))\n",
    "        MACCS_ids     = np.zeros((20,))\n",
    "        MACCS_ids[0]  = Descriptors.MolMR(mol)\n",
    "        MACCS_ids[1]  = Descriptors.MolLogP(mol)\n",
    "        MACCS_ids[2]  = Descriptors.MolWt(mol)\n",
    "        MACCS_ids[3]  = Descriptors.NumRotatableBonds(mol)\n",
    "        MACCS_ids[4]  = Descriptors.NumAliphaticRings(mol)\n",
    "        MACCS_ids[5]  = MACCS[108]\n",
    "        MACCS_ids[6]  = Descriptors.NumAromaticRings(mol)\n",
    "        MACCS_ids[7]  = MACCS[98]\n",
    "        MACCS_ids[8]  = Descriptors.NumSaturatedRings(mol)\n",
    "        MACCS_ids[9]  = MACCS[137]\n",
    "        MACCS_ids[10] = MACCS[136]\n",
    "        MACCS_ids[11] = MACCS[145]\n",
    "        MACCS_ids[12] = MACCS[116]\n",
    "        MACCS_ids[13] = MACCS[141]\n",
    "        MACCS_ids[14] = MACCS[89]\n",
    "        MACCS_ids[15] = MACCS[50]\n",
    "        MACCS_ids[16] = MACCS[160]\n",
    "        MACCS_ids[17] = MACCS[121]\n",
    "        MACCS_ids[18] = MACCS[149]\n",
    "        MACCS_ids[19] = MACCS[161]\n",
    "\n",
    "        if max_MolMR < MACCS_ids[0]:\n",
    "            max_MolMR = MACCS_ids[0]\n",
    "        if min_MolMR > MACCS_ids[0]:\n",
    "            min_MolMR = MACCS_ids[0]\n",
    "\n",
    "        if max_MolLogP < MACCS_ids[1]:\n",
    "            max_MolLogP = MACCS_ids[1]\n",
    "        if min_MolLogP > MACCS_ids[1]:\n",
    "            min_MolLogP = MACCS_ids[1]\n",
    "\n",
    "        if max_MolWt < MACCS_ids[2]:\n",
    "            max_MolWt = MACCS_ids[2]\n",
    "        if min_MolWt > MACCS_ids[2]:\n",
    "            min_MolWt = MACCS_ids[2]\n",
    "\n",
    "        if max_NumRotatableBonds < MACCS_ids[3]:\n",
    "            max_NumRotatableBonds = MACCS_ids[3]\n",
    "        if min_NumRotatableBonds > MACCS_ids[3]:\n",
    "            min_NumRotatableBonds = MACCS_ids[3]\n",
    "\n",
    "        if max_NumAliphaticRings < MACCS_ids[4]:\n",
    "            max_NumAliphaticRings = MACCS_ids[4]\n",
    "        if min_NumAliphaticRings > MACCS_ids[4]:\n",
    "            min_NumAliphaticRings = MACCS_ids[4]\n",
    "\n",
    "        if max_NumAromaticRings < MACCS_ids[6]:\n",
    "            max_NumAromaticRings = MACCS_ids[6]\n",
    "        if min_NumAromaticRings > MACCS_ids[6]:\n",
    "            min_NumAromaticRings = MACCS_ids[6]\n",
    "\n",
    "        if max_NumSaturatedRings < MACCS_ids[8]:\n",
    "            max_NumSaturatedRings = MACCS_ids[8]\n",
    "        if min_NumSaturatedRings > MACCS_ids[8]:\n",
    "            min_NumSaturatedRings = MACCS_ids[8]\n",
    "\n",
    "        MACCS_list.append(MACCS_ids)\n",
    "\n",
    "    dir_input = (f'{file_prefix}_pathway/input'+str(radius)+'/')\n",
    "    os.makedirs(dir_input, exist_ok=True)\n",
    "\n",
    "    for n in range(N):\n",
    "        for b in range(20):\n",
    "            if b==0:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_MolMR)/(max_MolMR-min_MolMR)\n",
    "            elif b==1:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_MolLogP)/(max_MolMR-min_MolLogP)\n",
    "            elif b==2:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_MolWt)/(max_MolMR-min_MolWt)\n",
    "            elif b==3:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_NumRotatableBonds)/(max_MolMR-min_NumRotatableBonds)\n",
    "            elif b==4:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_NumAliphaticRings)/(max_MolMR-min_NumAliphaticRings)\n",
    "            elif b==6:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_NumAromaticRings)/(max_MolMR-min_NumAromaticRings)\n",
    "            elif b==8:\n",
    "                MACCS_list[n][b] = (MACCS_list[n][b]-min_NumSaturatedRings)/(max_NumSaturatedRings-min_NumSaturatedRings)\n",
    "\n",
    "    np.save(dir_input + 'molecules', np.array(Molecules, dtype=object))\n",
    "    np.save(dir_input + 'adjacencies', np.array(Adjacencies, dtype=object))\n",
    "    np.save(dir_input + 'properties', np.array(Properties, dtype=object))\n",
    "    np.save(dir_input + 'maccs', np.asarray(MACCS_list))\n",
    "\n",
    "    dump_dictionary(fingerprint_dict, dir_input + 'fingerprint_dict.pickle')\n",
    "\n",
    "    print('The preprocess has finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "142c344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(file_prefix, radius):\n",
    "    dir_input = (f'{file_prefix}_pathway/input'+str(radius)+'/')\n",
    "\n",
    "    molecules    = load_tensor(dir_input + 'molecules', torch.LongTensor)\n",
    "    adjacencies  = load_numpy(dir_input + 'adjacencies')\n",
    "\n",
    "    import numpy as np\n",
    "    properties = np.load(dir_input + 'properties.npy', allow_pickle=True)  # Load as object\n",
    "    properties = np.array([np.asarray(x, dtype=np.float32) for x in properties])  # Convert to float\n",
    "    t_properties = torch.FloatTensor(properties)  # Now compatible\n",
    "    t_properties = t_properties.to(device)\n",
    "    maccs        = load_numpy(dir_input + 'maccs')\n",
    "\n",
    "    # with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "    #     fingerprint_dict = pickle.load(f)\n",
    "\n",
    "    dataset = list(zip(molecules, adjacencies, t_properties, maccs))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file_with_smiles('train', splits[0][0][0].merge([splits[0][0][1]]))\n",
    "create_file_with_smiles('test', splits[0][0][2])\n",
    "generate_features('train')\n",
    "generate_features('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_features('train', 2)\n",
    "test_dataset = load_features('test', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "model   = PathwayPredictor().to(device)\n",
    "trainer = Trainer(model)\n",
    "tester  = Tester(model)\n",
    "\n",
    "dir_output = ('pathway/output/')\n",
    "os.makedirs(dir_output, exist_ok=True)\n",
    "\n",
    "print('Training...')\n",
    "print('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC_test \\t Precision \\t Recall')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    if (epoch+1) % decay_interval == 0:\n",
    "        trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "    loss    = trainer.train(train_dataset)\n",
    "\n",
    "    lr_rate = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    end  = timeit.default_timer()\n",
    "    time = end - start\n",
    "\n",
    "    print('%d \\t %.4f \\t %.4f' %(epoch, time, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = list(zip(*test_dataset[:]))\n",
    "\n",
    "sel_maccs            = torch.FloatTensor(data_batch[-1])\n",
    "inputs, t_properties = data_batch[:-2], torch.cat(data_batch[-2])\n",
    "z_properties         = model.forward(inputs, sel_maccs)\n",
    "\n",
    "p_properties = torch.sigmoid(z_properties)\n",
    "\n",
    "p_properties = p_properties.data.to('cpu').numpy()\n",
    "t_properties = t_properties.data.to('cpu').numpy()\n",
    "\n",
    "p_properties[p_properties<0.5]  = 0\n",
    "p_properties[p_properties>=0.5] = 1\n",
    "\n",
    "y_true = t_properties\n",
    "y_pred = p_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6260788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7908363669781282"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "weighted_f1_scores = []\n",
    "weighted_precision_scores = []\n",
    "weighted_recall_scores = []\n",
    "\n",
    "f1_score_ = f1_score(y_pred=y_pred, y_true=y_true, average='macro')\n",
    "recall_score_ = recall_score(y_pred=y_pred, y_true=y_true, average='macro')\n",
    "precision_score_ = precision_score(y_pred=y_pred, y_true=y_true, average='macro')\n",
    "\n",
    "wf1_score = f1_score(y_pred=y_pred, y_true=y_true, average='weighted')\n",
    "wrecall_score = recall_score(y_pred=y_pred, y_true=y_true, average='weighted')\n",
    "wprecision_score = precision_score(y_pred=y_pred, y_true=y_true, average='weighted')\n",
    "\n",
    "# export to a dataframe and to a csv file\n",
    "import pandas as pd\n",
    "\n",
    "f1_scores.append(f1_score_)\n",
    "precision_scores.append(precision_score_)\n",
    "recall_scores.append(recall_score_)\n",
    "weighted_f1_scores.append(wf1_score)\n",
    "weighted_precision_scores.append(wprecision_score)\n",
    "weighted_recall_scores.append(wrecall_score)\n",
    "\n",
    "results = pd.DataFrame({'f1_score': f1_scores, 'precision': precision_scores, 'recall': recall_scores,\n",
    "                        'weighted_f1_score': weighted_f1_scores, 'weighted_precision': weighted_precision_scores,\n",
    "                        'weighted_recall': weighted_recall_scores})\n",
    "\n",
    "results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950680b3",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 2\n",
    "\n",
    "with open('kegg_smiles.txt', 'r') as f:\n",
    "    data_list = f.read().strip().split('\\n')\n",
    "\n",
    "\"\"\"Exclude the data contains \".\" in the smiles, which correspond to non-bonds\"\"\"\n",
    "data_list = list(filter(lambda x: '.' not in x.strip().split()[0], data_list))\n",
    "N = len(data_list)\n",
    "\n",
    "print('Total number of molecules : %d' %(N))\n",
    "\n",
    "atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "\n",
    "Molecules, Adjacencies, Properties, MACCS_list = [], [], [], []\n",
    "\n",
    "max_MolMR, min_MolMR     = -1000, 1000\n",
    "max_MolLogP, min_MolLogP = -1000, 1000\n",
    "max_MolWt, min_MolWt     = -1000, 1000\n",
    "max_NumRotatableBonds, min_NumRotatableBonds = -1000, 1000\n",
    "max_NumAliphaticRings, min_NumAliphaticRings = -1000, 1000\n",
    "max_NumAromaticRings, min_NumAromaticRings   = -1000, 1000\n",
    "max_NumSaturatedRings, min_NumSaturatedRings = -1000, 1000\n",
    "\n",
    "for no, data in enumerate(data_list):\n",
    "\n",
    "    print('/'.join(map(str, [no+1, N])))\n",
    "\n",
    "    smiles, property_indices = data.strip().split('\\t')\n",
    "    property_s = property_indices.strip().split(',')\n",
    "\n",
    "    property = np.zeros((1,21))\n",
    "    for prop in property_s:\n",
    "        property[0,int(prop)] = 1\n",
    "\n",
    "    Properties.append(property)\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atoms = create_atoms(mol)\n",
    "    i_jbond_dict = create_ijbonddict(mol)\n",
    "\n",
    "    fingerprints = create_fingerprints(atoms, i_jbond_dict, radius)\n",
    "    Molecules.append(fingerprints)\n",
    "\n",
    "    adjacency = create_adjacency(mol)\n",
    "    Adjacencies.append(adjacency)\n",
    "\n",
    "    MACCS         = MACCSkeys.GenMACCSKeys(Chem.MolFromSmiles(smiles))\n",
    "    MACCS_ids     = np.zeros((20,))\n",
    "    MACCS_ids[0]  = Descriptors.MolMR(mol)\n",
    "    MACCS_ids[1]  = Descriptors.MolLogP(mol)\n",
    "    MACCS_ids[2]  = Descriptors.MolWt(mol)\n",
    "    MACCS_ids[3]  = Descriptors.NumRotatableBonds(mol)\n",
    "    MACCS_ids[4]  = Descriptors.NumAliphaticRings(mol)\n",
    "    MACCS_ids[5]  = MACCS[108]\n",
    "    MACCS_ids[6]  = Descriptors.NumAromaticRings(mol)\n",
    "    MACCS_ids[7]  = MACCS[98]\n",
    "    MACCS_ids[8]  = Descriptors.NumSaturatedRings(mol)\n",
    "    MACCS_ids[9]  = MACCS[137]\n",
    "    MACCS_ids[10] = MACCS[136]\n",
    "    MACCS_ids[11] = MACCS[145]\n",
    "    MACCS_ids[12] = MACCS[116]\n",
    "    MACCS_ids[13] = MACCS[141]\n",
    "    MACCS_ids[14] = MACCS[89]\n",
    "    MACCS_ids[15] = MACCS[50]\n",
    "    MACCS_ids[16] = MACCS[160]\n",
    "    MACCS_ids[17] = MACCS[121]\n",
    "    MACCS_ids[18] = MACCS[149]\n",
    "    MACCS_ids[19] = MACCS[161]\n",
    "\n",
    "    if max_MolMR < MACCS_ids[0]:\n",
    "        max_MolMR = MACCS_ids[0]\n",
    "    if min_MolMR > MACCS_ids[0]:\n",
    "        min_MolMR = MACCS_ids[0]\n",
    "\n",
    "    if max_MolLogP < MACCS_ids[1]:\n",
    "        max_MolLogP = MACCS_ids[1]\n",
    "    if min_MolLogP > MACCS_ids[1]:\n",
    "        min_MolLogP = MACCS_ids[1]\n",
    "\n",
    "    if max_MolWt < MACCS_ids[2]:\n",
    "        max_MolWt = MACCS_ids[2]\n",
    "    if min_MolWt > MACCS_ids[2]:\n",
    "        min_MolWt = MACCS_ids[2]\n",
    "\n",
    "    if max_NumRotatableBonds < MACCS_ids[3]:\n",
    "        max_NumRotatableBonds = MACCS_ids[3]\n",
    "    if min_NumRotatableBonds > MACCS_ids[3]:\n",
    "        min_NumRotatableBonds = MACCS_ids[3]\n",
    "\n",
    "    if max_NumAliphaticRings < MACCS_ids[4]:\n",
    "        max_NumAliphaticRings = MACCS_ids[4]\n",
    "    if min_NumAliphaticRings > MACCS_ids[4]:\n",
    "        min_NumAliphaticRings = MACCS_ids[4]\n",
    "\n",
    "    if max_NumAromaticRings < MACCS_ids[6]:\n",
    "        max_NumAromaticRings = MACCS_ids[6]\n",
    "    if min_NumAromaticRings > MACCS_ids[6]:\n",
    "        min_NumAromaticRings = MACCS_ids[6]\n",
    "\n",
    "    if max_NumSaturatedRings < MACCS_ids[8]:\n",
    "        max_NumSaturatedRings = MACCS_ids[8]\n",
    "    if min_NumSaturatedRings > MACCS_ids[8]:\n",
    "        min_NumSaturatedRings = MACCS_ids[8]\n",
    "\n",
    "    MACCS_list.append(MACCS_ids)\n",
    "\n",
    "dir_input = ('pathway/input'+str(radius)+'/')\n",
    "os.makedirs(dir_input, exist_ok=True)\n",
    "\n",
    "for n in range(N):\n",
    "    for b in range(20):\n",
    "        if b==0:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_MolMR)/(max_MolMR-min_MolMR)\n",
    "        elif b==1:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_MolLogP)/(max_MolMR-min_MolLogP)\n",
    "        elif b==2:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_MolWt)/(max_MolMR-min_MolWt)\n",
    "        elif b==3:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_NumRotatableBonds)/(max_MolMR-min_NumRotatableBonds)\n",
    "        elif b==4:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_NumAliphaticRings)/(max_MolMR-min_NumAliphaticRings)\n",
    "        elif b==6:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_NumAromaticRings)/(max_MolMR-min_NumAromaticRings)\n",
    "        elif b==8:\n",
    "            MACCS_list[n][b] = (MACCS_list[n][b]-min_NumSaturatedRings)/(max_NumSaturatedRings-min_NumSaturatedRings)\n",
    "\n",
    "np.save(dir_input + 'molecules', np.array(Molecules, dtype=object))\n",
    "np.save(dir_input + 'adjacencies', np.array(Adjacencies, dtype=object))\n",
    "np.save(dir_input + 'properties', np.array(Properties, dtype=object))\n",
    "np.save(dir_input + 'maccs', np.asarray(MACCS_list))\n",
    "\n",
    "dump_dictionary(fingerprint_dict, dir_input + 'fingerprint_dict.pickle')\n",
    "\n",
    "print('The preprocess has finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17f5d6",
   "metadata": {},
   "source": [
    "## Define GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2269f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathwayPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PathwayPredictor, self).__init__()\n",
    "        self.embed_atom = nn.Embedding(n_fingerprint, dim)\n",
    "        self.W_atom = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layer)])\n",
    "        self.W_property = nn.Linear(dim+extra_dim, 21)\n",
    "\n",
    "    \"\"\"Pad adjacency matrices for batch processing.\"\"\"\n",
    "    def pad(self, matrices, value):\n",
    "        sizes = [d.shape[0] for d in matrices]\n",
    "        D = sum(sizes)\n",
    "        pad_matrices = value + np.zeros((D, D))\n",
    "        m = 0\n",
    "        for i, d in enumerate(matrices):\n",
    "            s_i = sizes[i]\n",
    "            pad_matrices[m:m+s_i, m:m+s_i] = d\n",
    "            m += s_i\n",
    "        return torch.FloatTensor(pad_matrices).to(device)\n",
    "\n",
    "    def sum_axis(self, xs, axis):\n",
    "        y = list(map(lambda x: torch.sum(x, 0), torch.split(xs, axis)))\n",
    "        return torch.stack(y)\n",
    "\n",
    "    def update(self, xs, adjacency, i):\n",
    "        hs = torch.relu(self.W_atom[i](xs))\n",
    "        return torch.matmul(adjacency, hs)\n",
    "\n",
    "    def forward(self, inputs, sel_maccs):\n",
    "\n",
    "        atoms, adjacency = inputs\n",
    "\n",
    "        axis = list(map(lambda x: len(x), atoms))\n",
    "\n",
    "        atoms = torch.cat(atoms)\n",
    "\n",
    "        x_atoms = self.embed_atom(atoms)\n",
    "        adjacency = self.pad(adjacency, 0)\n",
    "\n",
    "        for i in range(layer):\n",
    "            x_atoms = self.update(x_atoms, adjacency, i)\n",
    "\n",
    "        extra_inputs = sel_maccs.to(device)\n",
    "        y_molecules = self.sum_axis(x_atoms, axis)\n",
    "\n",
    "        y_molecules = torch.cat((y_molecules,extra_inputs),1)\n",
    "        z_properties = self.W_property(y_molecules)\n",
    "\n",
    "        return z_properties\n",
    "\n",
    "    def __call__(self, data_batch, train=True):\n",
    "\n",
    "        sel_maccs = torch.FloatTensor(data_batch[-1])\n",
    "\n",
    "        inputs, t_properties = data_batch[:-2], torch.cat(data_batch[-2])\n",
    "\n",
    "        z_properties = self.forward(inputs, sel_maccs)\n",
    "\n",
    "        if train:\n",
    "            loss = F.binary_cross_entropy(torch.sigmoid(z_properties), t_properties)\n",
    "            return loss\n",
    "        else:\n",
    "            zs = torch.sigmoid(z_properties).to('cpu').data.numpy()\n",
    "            ts = t_properties.to('cpu').data.numpy()\n",
    "            scores = list(map(lambda x: x, zs))\n",
    "            labels = list(map(lambda x: (x>=0.5).astype(int), zs))\n",
    "            return scores, labels, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81791722",
   "metadata": {},
   "source": [
    "## Train and test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8e4afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, dataset_train):\n",
    "        np.random.shuffle(dataset_train)\n",
    "        N = len(dataset_train)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N, batch):\n",
    "            data_batch = list(zip(*dataset_train[i:i+batch]))\n",
    "            loss = self.model(data_batch)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total\n",
    "    \n",
    "    \n",
    "\n",
    "class Tester(object):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset_test):\n",
    "\n",
    "        N = len(dataset_test)\n",
    "        score_list, label_list, t_list = [], [], []\n",
    "\n",
    "        for i in range(0, N, batch):\n",
    "            data_batch = list(zip(*dataset_test[i:i+batch]))\n",
    "            scores, labels, ts = self.model(data_batch, train=False)\n",
    "            score_list = np.append(score_list, scores)\n",
    "            label_list = np.append(label_list, labels)\n",
    "            t_list = np.append(t_list, ts)\n",
    "\n",
    "        auc       = accuracy_score(t_list, label_list)\n",
    "        precision = precision_score(t_list, label_list)\n",
    "        recall    = recall_score(t_list, label_list)\n",
    "\n",
    "        return auc, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fbb4e",
   "metadata": {},
   "source": [
    "## Define GNN parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73fb4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim            = 50\n",
    "layer          = 2\n",
    "batch          = 10\n",
    "lr             = 1e-3\n",
    "lr_decay       = 0.75\n",
    "decay_interval = 20\n",
    "iteration      = 100\n",
    "extra_dim      = 20\n",
    "\n",
    "(dim, layer, batch, decay_interval, iteration, extra_dim) = map(int, [dim, layer, batch, decay_interval, iteration, extra_dim])\n",
    "lr, lr_decay = map(float, [lr, lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b01686ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494f776",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7b8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_input = ('pathway/input'+str(radius)+'/')\n",
    "\n",
    "molecules    = load_tensor(dir_input + 'molecules', torch.LongTensor)\n",
    "adjacencies  = load_numpy(dir_input + 'adjacencies')\n",
    "\n",
    "import numpy as np\n",
    "properties = np.load(dir_input + 'properties.npy', allow_pickle=True)  # Load as object\n",
    "properties = np.array([np.asarray(x, dtype=np.float32) for x in properties])  # Convert to float\n",
    "t_properties = torch.FloatTensor(properties)  # Now compatible\n",
    "t_properties = t_properties.to(device)\n",
    "maccs        = load_numpy(dir_input + 'maccs')\n",
    "\n",
    "with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "    fingerprint_dict = pickle.load(f)\n",
    "\n",
    "dataset = list(zip(molecules, adjacencies, t_properties, maccs))\n",
    "dataset = shuffle_dataset(dataset, 1234)\n",
    "dataset_train, dataset_   = split_dataset(dataset, 0.8)\n",
    "dataset_dev, dataset_test = split_dataset(dataset_, 0.5)\n",
    "\n",
    "fingerprint_dict = load_pickle(dir_input + 'fingerprint_dict.pickle')\n",
    "unknown          = 100\n",
    "n_fingerprint    = len(fingerprint_dict) + unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de35cb3",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "885c5620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch \t Time(sec) \t Loss_train \t AUC_dev \t AUC_test \t Precision \t Recall\n",
      "0 \t 0.7159 \t 29.1145 \t 0.9543 \t 0.9516 \t 1.0000 \t 0.0922\n",
      "1 \t 1.1098 \t 15.2356 \t 0.9634 \t 0.9618 \t 0.8846 \t 0.3262\n",
      "2 \t 1.4988 \t 11.2653 \t 0.9661 \t 0.9709 \t 0.9444 \t 0.4823\n",
      "3 \t 1.8892 \t 8.9560 \t 0.9707 \t 0.9701 \t 0.8605 \t 0.5248\n",
      "4 \t 2.2960 \t 7.4958 \t 0.9768 \t 0.9773 \t 0.9010 \t 0.6454\n",
      "5 \t 2.7057 \t 6.3313 \t 0.9775 \t 0.9796 \t 0.9065 \t 0.6879\n",
      "6 \t 3.0467 \t 5.4742 \t 0.9787 \t 0.9785 \t 0.8962 \t 0.6738\n",
      "7 \t 3.3900 \t 4.8613 \t 0.9813 \t 0.9796 \t 0.8783 \t 0.7163\n",
      "8 \t 3.7293 \t 4.5035 \t 0.9844 \t 0.9834 \t 0.8943 \t 0.7801\n",
      "9 \t 4.0696 \t 4.0357 \t 0.9806 \t 0.9800 \t 0.8860 \t 0.7163\n",
      "10 \t 4.4082 \t 3.8019 \t 0.9859 \t 0.9837 \t 0.9224 \t 0.7589\n",
      "11 \t 4.7509 \t 3.4490 \t 0.9836 \t 0.9830 \t 0.8934 \t 0.7730\n",
      "12 \t 5.0913 \t 3.1827 \t 0.9810 \t 0.9830 \t 0.8692 \t 0.8014\n",
      "13 \t 5.4436 \t 2.9811 \t 0.9836 \t 0.9849 \t 0.9106 \t 0.7943\n",
      "14 \t 5.8361 \t 2.8467 \t 0.9832 \t 0.9853 \t 0.9180 \t 0.7943\n",
      "15 \t 6.2048 \t 2.6326 \t 0.9848 \t 0.9853 \t 0.9113 \t 0.8014\n",
      "16 \t 6.5540 \t 2.6126 \t 0.9844 \t 0.9872 \t 0.9213 \t 0.8298\n",
      "17 \t 6.9087 \t 2.3761 \t 0.9836 \t 0.9864 \t 0.9200 \t 0.8156\n",
      "18 \t 7.2965 \t 2.2719 \t 0.9848 \t 0.9898 \t 0.9254 \t 0.8794\n",
      "19 \t 7.6430 \t 2.0222 \t 0.9867 \t 0.9879 \t 0.9098 \t 0.8582\n",
      "20 \t 7.9888 \t 1.9888 \t 0.9848 \t 0.9853 \t 0.8643 \t 0.8582\n",
      "21 \t 8.3315 \t 1.8831 \t 0.9863 \t 0.9883 \t 0.9297 \t 0.8440\n",
      "22 \t 8.6881 \t 1.7669 \t 0.9855 \t 0.9872 \t 0.8963 \t 0.8582\n",
      "23 \t 9.0385 \t 1.7484 \t 0.9855 \t 0.9883 \t 0.9231 \t 0.8511\n",
      "24 \t 9.4063 \t 1.6638 \t 0.9832 \t 0.9890 \t 0.9118 \t 0.8794\n",
      "25 \t 9.7896 \t 1.5833 \t 0.9863 \t 0.9860 \t 0.9000 \t 0.8298\n",
      "26 \t 10.1747 \t 1.5311 \t 0.9836 \t 0.9864 \t 0.8832 \t 0.8582\n",
      "27 \t 10.5552 \t 1.5044 \t 0.9851 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "28 \t 10.9215 \t 1.4640 \t 0.9855 \t 0.9868 \t 0.9077 \t 0.8369\n",
      "29 \t 11.3100 \t 1.4140 \t 0.9863 \t 0.9875 \t 0.9030 \t 0.8582\n",
      "30 \t 11.6987 \t 1.4167 \t 0.9867 \t 0.9883 \t 0.9297 \t 0.8440\n",
      "31 \t 12.0963 \t 1.3314 \t 0.9859 \t 0.9894 \t 0.9124 \t 0.8865\n",
      "32 \t 12.4738 \t 1.2741 \t 0.9851 \t 0.9875 \t 0.8913 \t 0.8723\n",
      "33 \t 12.8581 \t 1.2523 \t 0.9863 \t 0.9883 \t 0.9231 \t 0.8511\n",
      "34 \t 13.2453 \t 1.2318 \t 0.9855 \t 0.9875 \t 0.9091 \t 0.8511\n",
      "35 \t 13.6318 \t 1.2540 \t 0.9859 \t 0.9890 \t 0.9242 \t 0.8652\n",
      "36 \t 14.0151 \t 1.2371 \t 0.9863 \t 0.9872 \t 0.9147 \t 0.8369\n",
      "37 \t 14.3999 \t 1.0801 \t 0.9863 \t 0.9875 \t 0.9286 \t 0.8298\n",
      "38 \t 14.7676 \t 1.1440 \t 0.9848 \t 0.9856 \t 0.8931 \t 0.8298\n",
      "39 \t 15.1396 \t 1.0150 \t 0.9855 \t 0.9883 \t 0.9231 \t 0.8511\n",
      "40 \t 15.5095 \t 1.1893 \t 0.9851 \t 0.9864 \t 0.9268 \t 0.8085\n",
      "41 \t 15.8890 \t 0.9416 \t 0.9851 \t 0.9894 \t 0.9248 \t 0.8723\n",
      "42 \t 16.2637 \t 0.9313 \t 0.9851 \t 0.9879 \t 0.9225 \t 0.8440\n",
      "43 \t 16.6368 \t 0.9495 \t 0.9874 \t 0.9890 \t 0.9242 \t 0.8652\n",
      "44 \t 17.0153 \t 0.8526 \t 0.9855 \t 0.9890 \t 0.9179 \t 0.8723\n",
      "45 \t 17.4005 \t 0.8802 \t 0.9851 \t 0.9875 \t 0.9219 \t 0.8369\n",
      "46 \t 17.7734 \t 0.8172 \t 0.9848 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "47 \t 18.1792 \t 0.8916 \t 0.9859 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "48 \t 18.5440 \t 0.8280 \t 0.9848 \t 0.9875 \t 0.9030 \t 0.8582\n",
      "49 \t 18.9055 \t 0.8581 \t 0.9851 \t 0.9887 \t 0.9237 \t 0.8582\n",
      "50 \t 19.2697 \t 0.8054 \t 0.9851 \t 0.9875 \t 0.9091 \t 0.8511\n",
      "51 \t 19.6448 \t 0.7609 \t 0.9848 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "52 \t 20.0080 \t 0.8337 \t 0.9855 \t 0.9887 \t 0.9302 \t 0.8511\n",
      "53 \t 20.3676 \t 0.7589 \t 0.9855 \t 0.9872 \t 0.9084 \t 0.8440\n",
      "54 \t 20.7267 \t 0.7371 \t 0.9855 \t 0.9890 \t 0.9179 \t 0.8723\n",
      "55 \t 21.0855 \t 0.8474 \t 0.9851 \t 0.9860 \t 0.8714 \t 0.8652\n",
      "56 \t 21.4444 \t 0.7329 \t 0.9859 \t 0.9872 \t 0.9023 \t 0.8511\n",
      "57 \t 21.8097 \t 0.7360 \t 0.9848 \t 0.9875 \t 0.9030 \t 0.8582\n",
      "58 \t 22.1709 \t 0.7113 \t 0.9859 \t 0.9864 \t 0.8889 \t 0.8511\n",
      "59 \t 22.5353 \t 0.7155 \t 0.9851 \t 0.9872 \t 0.8963 \t 0.8582\n",
      "60 \t 22.8981 \t 0.6343 \t 0.9851 \t 0.9875 \t 0.9219 \t 0.8369\n",
      "61 \t 23.2623 \t 0.6669 \t 0.9870 \t 0.9879 \t 0.8921 \t 0.8794\n",
      "62 \t 23.6240 \t 0.6376 \t 0.9859 \t 0.9879 \t 0.9098 \t 0.8582\n",
      "63 \t 23.9921 \t 0.6184 \t 0.9867 \t 0.9875 \t 0.9219 \t 0.8369\n",
      "64 \t 24.3541 \t 0.6732 \t 0.9851 \t 0.9868 \t 0.9141 \t 0.8298\n",
      "65 \t 24.7144 \t 0.5791 \t 0.9848 \t 0.9875 \t 0.8971 \t 0.8652\n",
      "66 \t 25.0737 \t 0.6089 \t 0.9851 \t 0.9864 \t 0.9008 \t 0.8369\n",
      "67 \t 25.4336 \t 0.6083 \t 0.9855 \t 0.9864 \t 0.8889 \t 0.8511\n",
      "68 \t 25.7949 \t 0.6635 \t 0.9851 \t 0.9868 \t 0.8897 \t 0.8582\n",
      "69 \t 26.1564 \t 0.5767 \t 0.9851 \t 0.9868 \t 0.9077 \t 0.8369\n",
      "70 \t 26.5200 \t 0.6117 \t 0.9874 \t 0.9887 \t 0.9173 \t 0.8652\n",
      "71 \t 26.8560 \t 0.5838 \t 0.9863 \t 0.9860 \t 0.9062 \t 0.8227\n",
      "72 \t 27.1899 \t 0.6181 \t 0.9859 \t 0.9879 \t 0.9160 \t 0.8511\n",
      "73 \t 27.5337 \t 0.5702 \t 0.9859 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "74 \t 27.8824 \t 0.5631 \t 0.9851 \t 0.9868 \t 0.9015 \t 0.8440\n",
      "75 \t 28.2359 \t 0.5609 \t 0.9851 \t 0.9883 \t 0.9167 \t 0.8582\n",
      "76 \t 28.5795 \t 0.5706 \t 0.9867 \t 0.9872 \t 0.9147 \t 0.8369\n",
      "77 \t 28.9119 \t 0.5619 \t 0.9859 \t 0.9890 \t 0.9118 \t 0.8794\n",
      "78 \t 29.2575 \t 0.5840 \t 0.9848 \t 0.9875 \t 0.9030 \t 0.8582\n",
      "79 \t 29.6199 \t 0.5117 \t 0.9867 \t 0.9887 \t 0.9237 \t 0.8582\n",
      "80 \t 29.9715 \t 0.4835 \t 0.9855 \t 0.9872 \t 0.9023 \t 0.8511\n",
      "81 \t 30.3064 \t 0.4673 \t 0.9859 \t 0.9894 \t 0.9313 \t 0.8652\n",
      "82 \t 30.6593 \t 0.4927 \t 0.9870 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "83 \t 31.0058 \t 0.4882 \t 0.9863 \t 0.9868 \t 0.8955 \t 0.8511\n",
      "84 \t 31.3378 \t 0.4785 \t 0.9855 \t 0.9875 \t 0.9030 \t 0.8582\n",
      "85 \t 31.6724 \t 0.4648 \t 0.9859 \t 0.9879 \t 0.9098 \t 0.8582\n",
      "86 \t 32.0251 \t 0.4775 \t 0.9870 \t 0.9887 \t 0.9237 \t 0.8582\n",
      "87 \t 32.3833 \t 0.4712 \t 0.9855 \t 0.9872 \t 0.8963 \t 0.8582\n",
      "88 \t 32.7417 \t 0.4927 \t 0.9855 \t 0.9883 \t 0.9104 \t 0.8652\n",
      "89 \t 33.1008 \t 0.4631 \t 0.9870 \t 0.9875 \t 0.9091 \t 0.8511\n",
      "90 \t 33.4609 \t 0.4864 \t 0.9859 \t 0.9856 \t 0.8705 \t 0.8582\n",
      "91 \t 33.8198 \t 0.4623 \t 0.9855 \t 0.9887 \t 0.9302 \t 0.8511\n",
      "92 \t 34.1782 \t 0.5224 \t 0.9855 \t 0.9872 \t 0.9213 \t 0.8298\n",
      "93 \t 34.5357 \t 0.4858 \t 0.9851 \t 0.9879 \t 0.9098 \t 0.8582\n",
      "94 \t 34.8950 \t 0.4501 \t 0.9870 \t 0.9879 \t 0.9160 \t 0.8511\n",
      "95 \t 35.2586 \t 0.4766 \t 0.9859 \t 0.9883 \t 0.9167 \t 0.8582\n",
      "96 \t 35.6226 \t 0.4702 \t 0.9855 \t 0.9868 \t 0.9015 \t 0.8440\n",
      "97 \t 35.9864 \t 0.4423 \t 0.9855 \t 0.9856 \t 0.8872 \t 0.8369\n",
      "98 \t 36.3493 \t 0.4731 \t 0.9859 \t 0.9868 \t 0.8897 \t 0.8582\n",
      "99 \t 36.7105 \t 0.4170 \t 0.9867 \t 0.9879 \t 0.9098 \t 0.8582\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "model   = PathwayPredictor().to(device)\n",
    "trainer = Trainer(model)\n",
    "tester  = Tester(model)\n",
    "\n",
    "dir_output = ('pathway/output/')\n",
    "os.makedirs(dir_output, exist_ok=True)\n",
    "\n",
    "print('Training...')\n",
    "print('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC_test \\t Precision \\t Recall')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    if (epoch+1) % decay_interval == 0:\n",
    "        trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "    loss    = trainer.train(dataset_train)\n",
    "    auc_dev = tester.test(dataset_dev)[0]\n",
    "    auc_test, precision, recall = tester.test(dataset_test)\n",
    "\n",
    "    lr_rate = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    end  = timeit.default_timer()\n",
    "    time = end - start\n",
    "\n",
    "    print('%d \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f' %(epoch, time, loss, auc_dev, auc_test, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d19c73",
   "metadata": {},
   "source": [
    "#### Random evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba646911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "0.00\b 0.00\b 1.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\n",
      "\n",
      "0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.87\b 0.87\b 0.00\n",
      "\n",
      "0.00\b 0.00\b 0.00\b 0.00\b 1.00\b 0.00\b 0.00\b 0.00\b 0.02\b 0.00\b 0.17\n",
      "\n",
      "0.00\b 0.00\b 1.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\n",
      "\n",
      "0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 1.00\n",
      "\n",
      "0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 1.00\b 0.00\b 0.00\n",
      "\n",
      "1.00\b 0.00\b 0.00\b 0.00\b 0.02\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\n",
      "\n",
      "0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 1.00\b 0.00\n",
      "\n",
      "0.00\b 0.00\b 1.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\n",
      "\n",
      "0.00\b 0.02\b 0.01\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\b 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_batch = list(zip(*dataset_test[0:0+batch]))\n",
    "\n",
    "sel_maccs            = torch.FloatTensor(data_batch[-1])\n",
    "inputs, t_properties = data_batch[:-2], torch.cat(data_batch[-2])\n",
    "z_properties         = model.forward(inputs, sel_maccs)\n",
    "\n",
    "# True classes\n",
    "print(t_properties)\n",
    "\n",
    "# Predicted classes\n",
    "torch.set_printoptions(precision=2)\n",
    "p_properties = torch.sigmoid(z_properties)\n",
    "\n",
    "for j in range(batch):\n",
    "    print('%.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\b %.2f\\n' %(p_properties[j,0], \\\n",
    "    p_properties[j,1], p_properties[j,2], p_properties[j,3], p_properties[j,4], p_properties[j,5], p_properties[j,6], \\\n",
    "    p_properties[j,7], p_properties[j,8], p_properties[j,9], p_properties[j,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0261f07",
   "metadata": {},
   "source": [
    "### Class-wise statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b5d753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 statistics:\n",
      "Accuracy 0.9841, Precision 0.0000, Recall 0.0000\n",
      "\n",
      "Class 2 statistics:\n",
      "Accuracy 0.9841, Precision 1.0000, Recall 0.5000\n",
      "\n",
      "Class 3 statistics:\n",
      "Accuracy 0.9524, Precision 0.6364, Recall 0.7778\n",
      "\n",
      "Class 4 statistics:\n",
      "Accuracy 0.9683, Precision 0.3333, Recall 0.3333\n",
      "\n",
      "Class 5 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n",
      "Class 6 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n",
      "Class 7 statistics:\n",
      "Accuracy 1.0000, Precision 0.0000, Recall 0.0000\n",
      "\n",
      "Class 8 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n",
      "Class 9 statistics:\n",
      "Accuracy 0.9762, Precision 0.8000, Recall 0.8889\n",
      "\n",
      "Class 10 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n",
      "Class 11 statistics:\n",
      "Accuracy 0.9921, Precision 1.0000, Recall 0.9000\n",
      "\n",
      "Class 12 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n",
      "Class 13 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n",
      "Class 14 statistics:\n",
      "Accuracy 0.9762, Precision 0.9167, Recall 0.8462\n",
      "\n",
      "Class 15 statistics:\n",
      "Accuracy 0.9841, Precision 1.0000, Recall 0.7500\n",
      "\n",
      "Class 16 statistics:\n",
      "Accuracy 0.9683, Precision 0.9000, Recall 0.7500\n",
      "\n",
      "Class 17 statistics:\n",
      "Accuracy 0.9921, Precision 1.0000, Recall 0.9500\n",
      "\n",
      "Class 18 statistics:\n",
      "Accuracy 0.9921, Precision 1.0000, Recall 0.9000\n",
      "\n",
      "Class 19 statistics:\n",
      "Accuracy 0.9841, Precision 1.0000, Recall 0.6000\n",
      "\n",
      "Class 20 statistics:\n",
      "Accuracy 0.9921, Precision 0.8000, Recall 1.0000\n",
      "\n",
      "Class 21 statistics:\n",
      "Accuracy 1.0000, Precision 1.0000, Recall 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcapela/.local/share/mamba/envs/np_benchmark/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jcapela/.local/share/mamba/envs/np_benchmark/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "data_batch = list(zip(*dataset_test[:]))\n",
    "\n",
    "sel_maccs            = torch.FloatTensor(data_batch[-1])\n",
    "inputs, t_properties = data_batch[:-2], torch.cat(data_batch[-2])\n",
    "z_properties         = model.forward(inputs, sel_maccs)\n",
    "\n",
    "torch.set_printoptions(precision=2)\n",
    "p_properties = torch.sigmoid(z_properties)\n",
    "\n",
    "p_properties = p_properties.data.to('cpu').numpy()\n",
    "t_properties = t_properties.data.to('cpu').numpy()\n",
    "\n",
    "p_properties[p_properties<0.5]  = 0\n",
    "p_properties[p_properties>=0.5] = 1\n",
    "\n",
    "for c in range(21):\n",
    "    y_true = t_properties[:,c]\n",
    "    y_pred = p_properties[:,c]\n",
    "\n",
    "    auc       = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall    = recall_score(y_true, y_pred)\n",
    "\n",
    "    print('Class '+str(c+1)+' statistics:')\n",
    "    print('Accuracy %.4f, Precision %.4f, Recall %.4f\\n' %(auc, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097eb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "np_benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
